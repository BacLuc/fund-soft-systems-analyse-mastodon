{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Software Systems - SE Part I Assignment\n",
    "\n",
    "By Andy Wiemeyer and Lucius Bachmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tools\n",
    "* checkout repo\n",
    "* initialize Git utility\n",
    "* You can recreate the Repository object with other parameters to analyze different time periods.\n",
    "  The last year was used that the setup is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from datetime import datetime\n",
    "from os import path, mkdir\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pydriller import Repository, Git\n",
    "\n",
    "repo_remote_path = 'https://github.com/mastodon/mastodon.git'\n",
    "repo_path = 'mastodon'\n",
    "repo_checkout_path = f'{repo_path}/{repo_path}'\n",
    "filepath = 'app'\n",
    "\n",
    "if not path.exists(repo_path):\n",
    "    mkdir(repo_path)\n",
    "\n",
    "repo = Repository(repo_remote_path, clone_repo_to=repo_path, since=datetime.fromisoformat('2022-10-01'), filepath=filepath)\n",
    "# clone repo if necessary\n",
    "for commit in repo.traverse_commits():\n",
    "    break\n",
    "git = Git(repo_checkout_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkout repo at tag v3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = git.get_commit_from_tag('v3.5.3')\n",
    "git.checkout(tag.hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Complexity Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You must consider only the app folder from the Mastodon repository\n",
    "(i.e., https://github.com/mastodon/mastodon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> nothing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decide on the granularity of your analysis of software entities (e.g., source code\n",
    "files); describe why you selected this specific granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis the granularity of source code files is used. It is an easy unit to perform measurements on. The mastodon repository contains a ruby on rails application with some javascript for the frontend. In both languages it's possible to define multiple classes in one file. Without performing a programming language specific analysis, it's not possible to measure smaller units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a list of all these entities, as they appear in the latest stable release of\n",
    "Mastodon (i.e., tag v3.5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_file_paths_all_dirs = git.files() #full path of all files in the analyzed commit\n",
    "subdirectory_start = \"/\" + repo_checkout_path + \"/\"\n",
    "full_file_paths = [path for path in full_file_paths_all_dirs if subdirectory_start+\"app/\" in path]\n",
    "subdirectory_start_index = full_file_paths[0].find(subdirectory_start) + len(subdirectory_start)\n",
    "subdirectory_prefix = full_file_paths[0][:subdirectory_start_index]#used later\n",
    "file_paths = [path[subdirectory_start_index:] for path in full_file_paths] #paths relative to analyzed subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [path[max(0,path.rfind(\"/\"))+1:] for path in full_file_paths]\n",
    "print(f\"Amount of different files with equal name: {len(file_names)-len(set(file_names))}\")\n",
    "counted_file_names = {}\n",
    "for name in file_names:\n",
    "    if name not in counted_file_names:\n",
    "        counted_file_names[name] = 1\n",
    "    else:\n",
    "        counted_file_names[name] += 1\n",
    "print([f\"{name}: {count}\" for name, count in counted_file_names.items() if count>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of different files have the same name (see above) so we use the paths to identify files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the file endings of the paths used in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "\n",
    "extensions = Counter([path.splitext(entity)[1] for entity in file_paths])\n",
    "\n",
    "colname_nr_of_occurences = 'nr of occurrences'\n",
    "df_extensions = DataFrame(extensions.items(), columns=['file ending', colname_nr_of_occurences])\n",
    "display(df_extensions.sort_values(by=colname_nr_of_occurences, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Decide on the type of complexity you want to measure for your software entities\n",
    "and explain why you selected this type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which metric would be a good indicator for complexity, a file was chosen to show the metrics the lizard library provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lizard\n",
    "\n",
    "filename = 'mastodon/mastodon/app/workers/scheduler/indexing_scheduler.rb'\n",
    "file = open(filename, mode='r')\n",
    "analysis = lizard.analyze_file.analyze_source_code(filename, file.read())\n",
    "print(f'of file {filename}')\n",
    "print(f'nr of functions: {len(analysis.function_list)}')\n",
    "print(f'cyclomatic complexity: {analysis.CCN}')\n",
    "print(f'lines of code: {analysis.nloc}')\n",
    "print(f'token_count: {analysis.token_count}')\n",
    "try:\n",
    "    print(f'deepest nesting level: {analysis.ND}')\n",
    "except AttributeError:\n",
    "    print(f'deepest nesting level threw an error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we learned that the number of lines of code is a proven measurement for complexity.\n",
    "Pydriller already measures this, thus the number of lines of code is used as measurement for complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Decide on a timeframe on which you want to base your analysis and explain the\n",
    "rationale of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, the activity on the repository is analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repository(repo_remote_path, clone_repo_to=repo_path)\n",
    "commit_dates = []\n",
    "\n",
    "for commit in repo.traverse_commits():\n",
    "    commit_dates.append(commit.committer_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "df_commit_dates = DataFrame(Counter(commit_dates).items(), columns=['date', 'nr_of_commits'])\n",
    "line = px.line(df_commit_dates, x='date', y='nr_of_commits')\n",
    "line.update_layout(font_size=18)\n",
    "line.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the development activity had a peak in 2017, and would be a little higher in the year 2020.\n",
    "Because we saw that with a timeframe between 2019 and 2022 we see a significant number of defects,\n",
    "we chose the timeframe of the last 3 years.\n",
    "\n",
    "If a complexity hotspot was found which was created a long time ago and\n",
    "was not changed for a long time, then the analysis would not add value.\n",
    "This analysis focuses on the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = datetime.fromisoformat('2019-11-08')\n",
    "to = datetime.fromisoformat('2022-11-08')\n",
    "\n",
    "repo = Repository(repo_remote_path, clone_repo_to=repo_path, since=since, to=to, filepath=filepath)\n",
    "git = Git(repo_checkout_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For each entity in the system, measure its complexity and the number of changes\n",
    "(in the given timeframe). Merge these two pieces of information together to cre-\n",
    "ate a candidate list of problematic hotspots in the app part of Mastodon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% compute results\n"
    }
   },
   "outputs": [],
   "source": [
    "path_to_results = './analysis_data.csv'\n",
    "if not os.path.isfile(path_to_results): #check if we have already saved the results\n",
    "    analysis_df = pd.DataFrame( #create dataframe with\n",
    "        {'Name':file_names, 'Full Path':full_file_paths}, #data columns\n",
    "        index=file_paths) #and index\n",
    "\n",
    "    #Compute complexities\n",
    "    complexity_comp_exceptions = {} #to analyse what caused error (mostly images)\n",
    "    for idx in analysis_df.index:\n",
    "        try:\n",
    "            with open(analysis_df.loc[idx, 'Full Path'], 'r') as file:\n",
    "                analysis_df.loc[idx, 'Complexity'] = len(file.readlines())\n",
    "        except Exception as e:\n",
    "            complexity_comp_exceptions[idx]=e\n",
    "    analysis_df.to_csv(path_to_results)\n",
    "\n",
    "    # Count amount of times changed\n",
    "    analysis_df['CommitPath'] = analysis_df.index #For tracking the path that a file has in the current commit if it was moved\n",
    "    analysis_df['Amount of changes'] = 0 #set change counter to 0\n",
    "    for commit in reversed(list(repo.traverse_commits())):\n",
    "        for file in commit.modified_files:\n",
    "            idx = analysis_df[analysis_df['CommitPath']==file.new_path].index\n",
    "            analysis_df.loc[idx, 'Amount of changes'] += 1\n",
    "            analysis_df.loc[idx, 'CommitPath'] = file.old_path #update commit path\n",
    "    analysis_df['Initial Path'] = analysis_df['CommitPath']\n",
    "    analysis_df.drop(columns='CommitPath', inplace=True) #get rid of temporary indexing\n",
    "    analysis_df.to_csv(path_to_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% load results\n"
    }
   },
   "outputs": [],
   "source": [
    "analysis_df = pd.read_csv(path_to_results, index_col=[0])\n",
    "# show results\n",
    "#analysis_df['Complexity'].sort_values(ascending=False)[:10] #Print top 10 entries\n",
    "#analysis_df['Amount of changes'].sort_values(ascending=False)[:10] #Print top 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Visualize the hotspots with a visualization of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_hist = px.histogram(analysis_df, x='Complexity', hover_data={\"path\": analysis_df.index})\n",
    "complexity_hist.show() #exponential dist in complexity -> use log axis\n",
    "change_amount_hist = px.histogram(analysis_df, x='Amount of changes', hover_data={\"path\": analysis_df.index})\n",
    "change_amount_hist.show() #exponential dist in amount of changes -> use log axis\n",
    "\n",
    "comparison_scatter = px.scatter(analysis_df, x='Complexity', y='Amount of changes',\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter.show()\n",
    "\n",
    "comparison_scatter_log = px.scatter(analysis_df, x='Complexity', y='Amount of changes',\n",
    "                 log_x=True, log_y=True,\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter_log.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(analysis_df.sort_values(by=['Amount of changes','Complexity'], ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Analyze six candidate hotspots (not necessarily the top ones) through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "computes the complextity trends for a list of file paths\n",
    "return: dictionary with paths as key and a list containing a complexity measurements from oldest to newest commit as value\n",
    "\"\"\"\n",
    "def compute_complexity_trends(file_paths):\n",
    "    complexity_trends = {path: [] for path in file_paths}\n",
    "    commit_paths = {path:path for path in file_paths} #For tracking the path that a file has in the current commit if it was moved\n",
    "\n",
    "    for commit in reversed(list(repo.traverse_commits())):#TODO: use whole list\n",
    "        git.checkout(commit.hash)\n",
    "\n",
    "        #add trend values for current commit\n",
    "        for key_path, value_path in commit_paths.items():\n",
    "            try:\n",
    "                full_path = subdirectory_prefix+value_path\n",
    "                with open(full_path, 'r') as file:\n",
    "                    complexity_trends[key_path].insert(0,len(file.readlines()))\n",
    "            except TypeError as e:\n",
    "                if e.args[0] == 'can only concatenate str (not \"NoneType\") to str':\n",
    "                    complexity_trends[key_path].insert(0,0)\n",
    "                else:\n",
    "                    raise Exception\n",
    "            except Exception as e:\n",
    "                print(\"Issue with complexity trend computation: \", e)\n",
    "\n",
    "        #update commit paths\n",
    "        for file in commit.modified_files:\n",
    "            for key_path, value_path in commit_paths.items():\n",
    "                if value_path==file.new_path and file.new_path!=None:\n",
    "                    commit_paths[key_path]=file.old_path\n",
    "\n",
    "    git.checkout(tag.hash) #return to initial checkout\n",
    "    return complexity_trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Compute trends and add to dataframe\n"
    }
   },
   "outputs": [],
   "source": [
    "hotspot_candidates = [\n",
    "    'app/javascript/fonts/roboto/roboto-medium-webfont.svg', #Complexity outlier, never changes\n",
    "    'app/models/status.rb', #Very high change rate somewhat high complexity\n",
    "    'app/javascript/styles/mastodon/components.scss', #Very high complexity, very high amount of changes\n",
    "    'app/javascript/mastodon/locales/ca.json',\n",
    "    'app/helpers/application_helper.rb',\n",
    "    'app/services/activitypub/process_status_update_service.rb']\n",
    "complexity_trends = compute_complexity_trends(hotspot_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Visualize trends\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_complexity_trend(index):\n",
    "    label, complexity_list = list(complexity_trends.items())[index]\n",
    "    commits_num = list(range(len(complexity_list)))\n",
    "    complexity_trend_line = px.line(x=commits_num, y=complexity_list, title=f\"Complexity trend for {label}\",\n",
    "                                    labels={'x': 'number of commits', 'y': 'complexity [LOC]'})\n",
    "    complexity_trend_line.show()\n",
    "    print(f\"Final complexity: {int(analysis_df['Complexity'][label])} LOC\" )\n",
    "    print(f\"Amount of changes: {analysis_df['Amount of changes'][label]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_log(filepath: str) -> DataFrame:\n",
    "    global commit\n",
    "    repo_file = Repository(path_to_repo=repo_checkout_path, since=since, to=to,\n",
    "                           filepath=filepath)\n",
    "    git_log_file = []\n",
    "    for commit in repo_file.traverse_commits():\n",
    "        insertions = 0\n",
    "        deletions = 0\n",
    "        for file in commit.modified_files:\n",
    "            if file.new_path == filepath:\n",
    "                insertions = file.added_lines\n",
    "                deletions = file.deleted_lines\n",
    "        git_log_file.append((commit.hash[:7], commit.msg, insertions, deletions))\n",
    "    return DataFrame(git_log_file, columns=['hash', 'message', 'insertions', 'deletions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def show_stats_for_glob(glob_str: str):\n",
    "    def create_file_row(file_path: str):\n",
    "        analysis = lizard.analyze_file(file_path)\n",
    "        return file_path.replace(repo_checkout_path, ''), analysis.nloc\n",
    "    files = [create_file_row(filepath) for filepath in glob(f'{repo_checkout_path}/{glob_str}', recursive=True)]\n",
    "    display(DataFrame(files, columns=['filename', 'nloc']).sort_values(by=['nloc'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 1: app/javascript/fonts/roboto/roboto-medium-webfont.svg\n",
    "\n",
    "The 'roboto-medium-webfont.svg' file is an extreme outlier. It has very high complexity and never changes.\n",
    "From the file name we can easily see that it is not a troublesome hotspot.\n",
    "The file needs to have a lot of lines of code because it defines the shape of characters from a font.\n",
    "None of these lines ever need to change though. It is not a hotspot and we do not need to take any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 2: app/models/status.rb\n",
    "\n",
    "From the trend we see that the complexity increased gradually, with some decreases inbetween.\n",
    "Presumably these are refactorings, indicating that the file does indeed require some maintenance work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 3: app/javascript/styles/mastodon/components.scss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_log = get_git_log('app/javascript/styles/mastodon/components.scss')\n",
    "display(git_log[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the file is changed for unrelated features. This assumes that for a lot of css in this file\n",
    "for different responsibilities.\n",
    "This raises the question if the scss files are separated at all.\n",
    "For that the nloc of the scss files needs to be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_stats_for_glob('**/*.scss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can see that the scss is separated in some files, but the components.css is still\n",
    "a lot larger than all the other scss files.\n",
    "The commits 2de44d3 and 2de5128 show that 2 pull requests were needed to fix the same regression.\n",
    "The function `application_helper.rb::react_component` indicates that there are react components\n",
    "in this repository, which would allow to split and scope the scss. This was not done and thus\n",
    "the components.scss file is large and handles the styling concerns of more than one component.\n",
    "This already lead to regressions. Components.scss is a complexity hotspot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 4: app/javascript/mastodon/locales/ca.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ca.json and the other locales files contain some translations for the languages mastodon supports.\n",
    "So these files contain a lot of lines and change on 3 occasions:\n",
    "\n",
    "1. If a new translation key is added\n",
    "2. If the translation for a translation key is changed\n",
    "3. If the key for a translation needs to be changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_log = get_git_log('app/javascript/mastodon/locales/ca.json')\n",
    "display(git_log[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the commit messages we can see that mastodon uses Crowdin to manage their translations.\n",
    "It's good that the translations can be changed independently. On the other hand we see in commits\n",
    "1392741 and db04dfc we see that sometimes the authors forget to add the translation keys to the\n",
    "locales files. This is not really a hotspot, but shows that high number of lines of code and a\n",
    "high change rate do not always indicate a hotspot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 5: app/helpers/application_helper.rb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the name one does not really know what this module should do.\n",
    "The amount of changes is not large, but the file is in the upper right corner of the scatter plot above.\n",
    "Because the name does not really describe the responsibility of the module, an analysis of the method names was made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mastodon/mastodon/app/helpers/application_helper.rb'\n",
    "file = open(filename, mode='r')\n",
    "analysis = lizard.analyze_file.analyze_source_code(filename, file.read())\n",
    "functions = [function_info.name for function_info in analysis.function_list]\n",
    "display(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions seem to be helpers for rendering the ui, like `active_nav_class`, `active_link_to`, `friendly_number_to_human`, `favicon_path`\n",
    ", `react_component`.\n",
    "Others seem to contain business logic like `grouped_scopes`, `open_registrations?`.\n",
    "Its not the most important hotspot, but it would be good to split this module into separate modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 6: app/services/activitypub/process_status_update_service.rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file started with a high complexity, and its complexity stayed at the same level.\n",
    "The changes made to the file did not change the complexity drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_git_log('app/services/activitypub/process_status_update_service.rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The git log shows a lot of fixes in this file. This makes it a complexity hotspot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Temporal/Logical Coupling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine what could be cases of temporal/logical coupling and generate a list\n",
    "of candidates with a set of coupled entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we consider logical grouping by commits.\n",
    "According to the [guidelines for pull requrests](https://github.com/mastodon/mastodon/blob/97f657f8181dc24f6c30b6e9f0ce52df827ac90f/CONTRIBUTING.md#pull-requests),\n",
    "small pull requests should be preferred, thus it is assumed that they contain only the necessary changes for one \"Add\", \"Change\", \"Deprecate\", \"Remove\" or \"Fix\",\n",
    "and can be viewed as logically grouped. To merge the pull request, a squash commit is added to the main branch. So for this analysis we can use the commits\n",
    "on the main branch to detect logical coupling between files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the sets of logically coupled is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChangeSet:\n",
    "    files: Dict[str,int]\n",
    "    counter: int\n",
    "\n",
    "    def __init__(self, files: List[str]):\n",
    "        self.files={f:1 for f in files}\n",
    "        self.counter = 0\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(\", \".join(self.get_files()))\n",
    "\n",
    "\n",
    "    def __eq__(self, o: object) -> bool:\n",
    "        if not isinstance(o, ChangeSet):\n",
    "            return False\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return self.get_files() == o.get_files()\n",
    "\n",
    "    def to_tuple(self, total_commits_file1 : int, total_commits_file2 : int):\n",
    "        real_number_of_coupled_commits = self.counter / 2\n",
    "        file1_coupling = real_number_of_coupled_commits / total_commits_file1\n",
    "        file2_coupling = real_number_of_coupled_commits / total_commits_file2\n",
    "        max_coupling = max(file1_coupling, file2_coupling)\n",
    "        return self.get_files()[0], total_commits_file1, file1_coupling, self.get_files()[1], total_commits_file2, file2_coupling, real_number_of_coupled_commits, max_coupling\n",
    "\n",
    "    def get_files(self):\n",
    "        return sorted(self.files.keys())\n",
    "\n",
    "    def changeset_changed(self):\n",
    "        self.counter += 1\n",
    "changed_together_filename = './changed_together.csv'\n",
    "\n",
    "col_name_nr_changed_together = 'nr_changed_together'\n",
    "col_name_coupling_file1 = 'file1_coupling'\n",
    "col_name_coupling_file2 = 'file2_coupling'\n",
    "col_name_max_coupling = 'max_coupling'\n",
    "changed_together_columns = ['file1', 'file1_total_commits', col_name_coupling_file1 , 'file2', 'file2_total_commits', col_name_coupling_file2, col_name_nr_changed_together, col_name_max_coupling]\n",
    "dataframe_changed_together = DataFrame([], columns=changed_together_columns)\n",
    "\n",
    "if not path.exists(changed_together_filename):\n",
    "    files_changed_together : Dict[ChangeSet,ChangeSet] = {}\n",
    "    commits_on_file_counter: Dict[str,int] = {}\n",
    "    for commit in repo.traverse_commits():\n",
    "        for file in commit.modified_files:\n",
    "\n",
    "            if not file.new_path in commits_on_file_counter:\n",
    "                commits_on_file_counter[file.new_path] = 0\n",
    "            commits_on_file_counter[file.new_path] += 1\n",
    "\n",
    "            for changed_together in commit.modified_files:\n",
    "                if file.new_path is None or changed_together.new_path is None:\n",
    "                    continue\n",
    "                if not file.new_path.__contains__('app/') or not changed_together.new_path.__contains__('app'):\n",
    "                    continue\n",
    "                if not file.new_path == changed_together.new_path:\n",
    "                    change_set = ChangeSet([file.new_path, changed_together.new_path])\n",
    "                    if change_set in files_changed_together:\n",
    "                        change_set = files_changed_together[change_set]\n",
    "                    else:\n",
    "                        files_changed_together[change_set] = change_set\n",
    "                    change_set.changeset_changed()\n",
    "\n",
    "    changed_together_tuples = []\n",
    "    for change_set in files_changed_together.values():\n",
    "        total_commits_file1 = commits_on_file_counter[change_set.get_files()[0]]\n",
    "        total_commits_file2 = commits_on_file_counter[change_set.get_files()[1]]\n",
    "        changed_together_tuples.append(change_set.to_tuple(total_commits_file1, total_commits_file2))\n",
    "\n",
    "    dataframe_changed_together = DataFrame(changed_together_tuples, columns=changed_together_columns)\n",
    "    dataframe_changed_together = dataframe_changed_together.sort_values(by=[col_name_nr_changed_together], ascending=False)\n",
    "    dataframe_changed_together.to_csv(changed_together_filename)\n",
    "\n",
    "dataframe_changed_together = pd.read_csv(changed_together_filename, index_col=[0,1])\n",
    "\n",
    "\n",
    "\n",
    "print(f'most common changed together:')\n",
    "display(dataframe_changed_together[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the locales change often together. Thus, we need to remove the locales from the analysis.\n",
    "Then the data is filtered by files which are at least 5 times changed together, and sorted by the max_coupling value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "allowed_file_endings = ['.rb', '.js', '.haml', '.erb']\n",
    "\n",
    "def contains_allowed_file_type(change_set: List) -> bool:\n",
    "    for file in change_set:\n",
    "        ending = path.splitext(file)[1]\n",
    "        if allowed_file_endings.__contains__(ending):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "changed_together_filtered = [(row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8]) for row in dataframe_changed_together.to_records(index=True) if contains_allowed_file_type([row[1], row[4]])]\n",
    "dataframe_changed_together_filtered = DataFrame(changed_together_filtered, columns=changed_together_columns)\n",
    "dataframe_changed_together_filtered = dataframe_changed_together_filtered[dataframe_changed_together_filtered['nr_changed_together'] > 5].sort_values(by=[col_name_max_coupling], ascending=False)\n",
    "\n",
    "tabulate(dataframe_changed_together_filtered, headers=changed_together_columns, tablefmt=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualize these candidate sets of couple entities with a visualization of your\n",
    "choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_hist = px.histogram(dataframe_changed_together_filtered, x=col_name_nr_changed_together, log_y=True)\n",
    "complexity_hist.update_layout(font_size=18)\n",
    "complexity_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The histogram visualises that there is only one file pair of the analyzed file types (which excludes the locales), which was changed a lot together.\n",
    "The others were only changed together 6 times in the last 3 years."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_bar_plot = [(f'{row[1]} +  {row[4]}', row[8]) for row in dataframe_changed_together_filtered.to_records(index=True)]\n",
    "dataframe_for_bar_plot = DataFrame(list_for_bar_plot, columns=['files', col_name_max_coupling])\n",
    "\n",
    "bar = px.bar(dataframe_for_bar_plot[:20], x=col_name_max_coupling, y='files')\n",
    "bar.update_layout(font_size=18)\n",
    "bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this horizontal bar charts, the pairs are sorted by the maximal coupling value of the two files involved.\n",
    "There are the following groups of pairs:\n",
    "* Coupled frontend (javascript or haml files)\n",
    "* Coupled backend controllers\n",
    "* Coupled frontend state management files (for redux which we can see from the actions/compose.js and the actions/reducers.js files)\n",
    "* Coupled backend helpers with frontend they render"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For three set candidates in the list:\n",
    "• analyze and explain why these entities are coupled;\n",
    "• describe how important it would be to fix them, and any ideas for their\n",
    "improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canditate set 1 (app/controllers/follower_accounts_controller.rb, app/controllers/following_accounts_controller.rb):"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values retrieved for the set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tabulate(dataframe_changed_together_filtered[dataframe_changed_together_filtered['file1'] == 'app/controllers/follower_accounts_controller.rb'], headers=changed_together_columns, tablefmt=\"html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The joined git history:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def join_git_logs(git_log_1: DataFrame, git_log_2: DataFrame) -> DataFrame:\n",
    "    return git_log_1.set_index('hash').join(git_log_2.set_index('hash'), how='outer', rsuffix='_right')\n",
    "\n",
    "git_log_1 = get_git_log('app/controllers/follower_accounts_controller.rb')\n",
    "git_log_2 = get_git_log('app/controllers/following_accounts_controller.rb')\n",
    "display(join_git_logs(git_log_1, git_log_2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This git log suggests that the changing parts of the two files are copy pasted."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(os.system('git diff --no-index mastodon/mastodon/app/controllers/follower_accounts_controller.rb mastodon/mastodon/app/controllers/following_accounts_controller.rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 2 files are coupled because they need to fulfil the same responsibility in a different context.\n",
    "The coupling could be avoided if the common logic is extracted into a third class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canditate set 2 (app/javascript/mastodon/actions/compose.js, app/javascript/mastodon/reducers/compose.js):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tabulate(dataframe_changed_together_filtered[dataframe_changed_together_filtered['file1'] == 'app/javascript/mastodon/actions/compose.js'], headers=changed_together_columns, tablefmt=\"html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "git_log_1 = get_git_log('app/javascript/mastodon/actions/compose.js')\n",
    "git_log_2 = get_git_log('app/javascript/mastodon/reducers/compose.js')\n",
    "display(join_git_logs(git_log_1, git_log_2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The actions and the reducers are tightly coupled in the redux state management, because the reducers need to calculate the new\n",
    "state given the old state and the action.\n",
    "Another state management system might allow to put related concerns into the same file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate set 3 (app/helpers/statuses_helper.rb, app/views/statuses/_simple_status.html.haml):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tabulate(dataframe_changed_together_filtered[dataframe_changed_together_filtered['file1'] == 'app/helpers/statuses_helper.rb'], headers=changed_together_columns, tablefmt=\"html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "git_log_1 = get_git_log('app/helpers/statuses_helper.rb')\n",
    "git_log_2 = get_git_log('app/views/statuses/_simple_status.html.haml')\n",
    "display(join_git_logs(git_log_1, git_log_2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the statuses_helper provides logic for the haml template.\n",
    "Because the status_helper is only shared among 2 related templates, this is not an issue."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Decide on how you want to detect entities that had defects in the past (e.g.,\n",
    "commit message analysis vs. issue tracking system analysis) and motivate your\n",
    "choice.\n",
    "\n",
    "We decided to look at commit messages. Whenever a file was changed in a commit with message that contains a string like \"bug\" or \"fix\" (see \"indicator_list\" below), we increment a counter for that file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine defective hotspots among the entities in the timeframe that you pre-\n",
    "viously selected (i.e., consider only defects in the selected timeframe). What\n",
    "conclusions can you draw from this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indictor_list = [\"fix\", \"bug\"]\n",
    "if not 'Defects' in analysis_df: #check if we have already saved the results\n",
    "\n",
    "    #Compute defect count\n",
    "    analysis_df['CommitPath'] = analysis_df.index #For tracking the path that a file has in the current commit if it was moved\n",
    "    analysis_df['Defects'] = 0 #set change counter to 0\n",
    "\n",
    "    for commit in reversed(list(repo.traverse_commits())):\n",
    "        fixes_bug = any([indicator in commit.msg.lower() for indicator in indictor_list])\n",
    "        for file in commit.modified_files:\n",
    "            idx = analysis_df[analysis_df['CommitPath']==file.new_path].index\n",
    "            analysis_df.loc[idx, 'CommitPath'] = file.old_path #update commit path\n",
    "            if fixes_bug:\n",
    "                analysis_df.loc[idx, 'Defects'] += 1\n",
    "\n",
    "    analysis_df.drop(columns='CommitPath', inplace=True) #get rid of temporary indexing\n",
    "    analysis_df.to_csv(path_to_results)\n",
    "\n",
    "analysis_df['Defects'].sort_values(ascending=False)[:10] #Print top 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine complexity hotspots at the beginning of your timeframe, then corre-\n",
    "late them with the defects they have presented throughout the entire timeframe.\n",
    "Is there a correlation? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find complexity hotspots at the start\n",
    "initial_commit = list(repo.traverse_commits())[0]\n",
    "git.checkout(initial_commit.hash)# go to first commit\n",
    "\n",
    "complexity_comp_exceptions = {} #to analyse what caused error (mostly images)\n",
    "for idx in analysis_df.index:\n",
    "    try:\n",
    "        if type(analysis_df.loc[idx, 'Initial Path']) != str: #if the file is deleted, initial path is a float, nan\n",
    "            analysis_df.loc[idx, 'Initial Complexity'] = 0\n",
    "        else:\n",
    "            full_path = subdirectory_prefix+analysis_df.loc[idx, 'Initial Path']\n",
    "            with open(full_path, 'r') as file:\n",
    "                analysis_df.loc[idx, 'Initial Complexity'] = len(file.readlines())\n",
    "    except Exception as e:\n",
    "        complexity_comp_exceptions[idx]=e\n",
    "#print(\"Exceptions: \\n\", complexity_comp_exceptions)\n",
    "analysis_df.to_csv(path_to_results)\n",
    "\n",
    "git.checkout(tag.hash)# revert to latest commit\n",
    "display(analysis_df.sort_values(by=['Initial Complexity', 'Defects'], ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_scatter = px.scatter(analysis_df, x='Initial Complexity', y='Defects',\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter.show()\n",
    "\n",
    "comparison_scatter_zoomed = px.scatter(analysis_df, x='Initial Complexity', y='Defects',\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter_zoomed.update_yaxes(range = [0,30])\n",
    "comparison_scatter_zoomed.update_xaxes(range = [0,1100])\n",
    "comparison_scatter_zoomed.show()\n",
    "\n",
    "reduced_df = analysis_df.dropna(subset=['Initial Path'])\n",
    "reduced_df = reduced_df[~((reduced_df['Initial Path'].str.endswith('.scss'))|(reduced_df['Initial Path'].str.endswith('.json')))]\n",
    "comparison_scatter_zoomed_filtered = px.scatter(reduced_df, x='Initial Complexity', y='Defects',\n",
    "                 hover_data={\"path\": reduced_df.index})\n",
    "comparison_scatter_zoomed_filtered.update_yaxes(range = [0,30])\n",
    "comparison_scatter_zoomed_filtered.update_xaxes(range = [0,1100])\n",
    "comparison_scatter_zoomed_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that there is a slight correlation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. What conclusions can you draw from the relationship between defective hotspots\n",
    "and complexity hotspots in Mastodon? And on these two metrics in general?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
