{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Software Systems - SE Part I Assignment\n",
    "\n",
    "By Andy Wiemeyer and Lucius Bachmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tools\n",
    "* checkout repo\n",
    "* initialize Git utility\n",
    "* You can recreate the Repository object with other parameters to analyze different time periods.\n",
    "  The last year was used that the setup is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from datetime import datetime\n",
    "from pydriller import Repository, Git\n",
    "from os import path, mkdir\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "repo_remote_path = 'https://github.com/mastodon/mastodon.git'\n",
    "repo_path = 'mastodon'\n",
    "repo_checkout_path = f'{repo_path}/{repo_path}'\n",
    "filepath = 'app'\n",
    "\n",
    "since = datetime.fromisoformat('2021-11-08')\n",
    "to = datetime.fromisoformat('2022-11-08')\n",
    "\n",
    "if not path.exists(repo_path):\n",
    "    mkdir(repo_path)\n",
    "\n",
    "repo = Repository(repo_remote_path, clone_repo_to=repo_path, since=since, to=to, filepath=filepath)\n",
    "# clone repo if necessary\n",
    "for commit in repo.traverse_commits():\n",
    "    break\n",
    "git = Git(repo_checkout_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkout repo at tag v3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = git.get_commit_from_tag('v3.5.3')\n",
    "git.checkout(tag.hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Complexity Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You must consider only the app folder from the Mastodon repository\n",
    "(i.e., https://github.com/mastodon/mastodon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> nothing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decide on the granularity of your analysis of software entities (e.g., source code\n",
    "files); describe why you selected this specific granularity.\n",
    "\n",
    "We decided to look at single files TODO: motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a list of all these entities, as they appear in the latest stable release of\n",
    "Mastodon (i.e., tag v3.5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_file_paths_all_dirs = git.files() #full path of all files in the analyzed commit\n",
    "subdirectory_start = \"/\" + repo_checkout_path + \"/\"\n",
    "full_file_paths = [path for path in full_file_paths_all_dirs if subdirectory_start+\"app/\" in path]\n",
    "subdirectory_start_index = full_file_paths[0].find(subdirectory_start) + len(subdirectory_start)\n",
    "subdirectory_prefix = full_file_paths[0][:subdirectory_start_index]#used later\n",
    "file_paths = [path[subdirectory_start_index:] for path in full_file_paths] #paths relative to analyzed subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_names = [path[max(0,path.rfind(\"/\"))+1:] for path in full_file_paths]\n",
    "print(f\"Amount of different files with equal name: {len(file_names)-len(set(file_names))}\")\n",
    "counted_file_names = {}\n",
    "for name in file_names:\n",
    "    if name not in counted_file_names:\n",
    "        counted_file_names[name] = 1\n",
    "    else:\n",
    "        counted_file_names[name] += 1\n",
    "print([f\"{name}: {count}\" for name, count in counted_file_names.items() if count>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of different files have the same name (see above) so we use the paths to identify files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Decide on the type of complexity you want to measure for your software entities\n",
    "and explain why you selected this type.\n",
    "\n",
    "We decided at to look at the lines of code in a file TODO: explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Decide on a timeframe on which you want to base your analysis and explain the\n",
    "rationale of your choice.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For each entity in the system, measure its complexity and the number of changes\n",
    "(in the given timeframe). Merge these two pieces of information together to cre-\n",
    "ate a candidate list of problematic hotspots in the app part of Mastodon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_results = './analysis_data.csv'\n",
    "if not os.path.isfile(path_to_results): #check if we have already saved the results\n",
    "    analysis_df = pd.DataFrame( #create dataframe with\n",
    "        {'Name':file_names, 'Full Path':full_file_paths}, #data columns\n",
    "        index=file_paths) #and index\n",
    "\n",
    "    #Compute complexities\n",
    "    complexity_comp_exceptions = {} #to analyse what caused error (mostly images)\n",
    "    for idx in analysis_df.index:\n",
    "        try:\n",
    "            with open(analysis_df.loc[idx, 'Full Path'], 'r') as file:\n",
    "                analysis_df.loc[idx, 'Complexity'] = len(file.readlines())\n",
    "        except Exception as e:\n",
    "            complexity_comp_exceptions[idx]=e\n",
    "    analysis_df.to_csv(path_to_results)\n",
    "\n",
    "    # Count amount of times changed\n",
    "    analysis_df['CommitPath'] = analysis_df.index #For tracking the path that a file has in the current commit if it was moved\n",
    "    analysis_df['Amount of changes'] = 0 #set change counter to 0\n",
    "    for commit in reversed(list(repo.traverse_commits())):\n",
    "        for file in commit.modified_files:\n",
    "            idx = analysis_df[analysis_df['CommitPath']==file.new_path].index\n",
    "            analysis_df.loc[idx, 'Amount of changes'] += 1\n",
    "            analysis_df.loc[idx, 'CommitPath'] = file.old_path #update commit path\n",
    "    analysis_df.drop(columns='CommitPath', inplace=True) #get rid of temporary indexing\n",
    "    analysis_df.to_csv(path_to_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% load results\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis_df = pd.read_csv(path_to_results, index_col=[0])\n",
    "# show results\n",
    "#analysis_df['Complexity'].sort_values(ascending=False)[:10] #Print top 10 entries\n",
    "#analysis_df['Amount of changes'].sort_values(ascending=False)[:10] #Print top 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Visualize the hotspots with a visualization of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "complexity_hist = px.histogram(analysis_df, x='Complexity', hover_data={\"path\": analysis_df.index})\n",
    "complexity_hist.show() #exponential dist in complexity -> use log axis\n",
    "change_amount_hist = px.histogram(analysis_df, x='Amount of changes', hover_data={\"path\": analysis_df.index})\n",
    "change_amount_hist.show() #exponential dist in amount of changes -> use log axis\n",
    "\n",
    "comparison_scatter = px.scatter(analysis_df, x='Complexity', y='Amount of changes',\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter.show()\n",
    "\n",
    "comparison_scatter_log = px.scatter(analysis_df, x='Complexity', y='Amount of changes',\n",
    "                 log_x=True, log_y=True,\n",
    "                 hover_data={\"path\": analysis_df.index})\n",
    "comparison_scatter_log.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Analyze six candidate hotspots (not necessarily the top ones) through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "computes the complextity trends for a list of file paths\n",
    "return: dictionary with paths as key and a list containing a complexity measurements from oldest to newest commit as value\n",
    "\"\"\"\n",
    "def compute_complexity_trends(file_paths):\n",
    "    complexity_trends = {path: [] for path in file_paths}\n",
    "    commit_paths = {path:path for path in file_paths} #For tracking the path that a file has in the current commit if it was moved\n",
    "\n",
    "    for commit in reversed(list(repo.traverse_commits())):#TODO: use whole list\n",
    "        git.checkout(commit.hash)\n",
    "\n",
    "        #add trend values for current commit\n",
    "        for key_path, value_path in commit_paths.items():\n",
    "            try:\n",
    "                full_path = subdirectory_prefix+value_path\n",
    "                with open(full_path, 'r') as file:\n",
    "                    complexity_trends[key_path].insert(0,len(file.readlines()))\n",
    "            except Exception as e:\n",
    "                print(\"Issue with complexity trend computation: \", e)\n",
    "\n",
    "        #update commit paths\n",
    "        for file in commit.modified_files:\n",
    "            for key_path, value_path in commit_paths.items():\n",
    "                if value_path==file.new_path:\n",
    "                    commit_paths[key_path]=file.old_path\n",
    "\n",
    "    git.checkout(tag.hash) #return to initial checkout\n",
    "    return complexity_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Compute trends and add to dataframe\n"
    }
   },
   "outputs": [],
   "source": [
    "hotspot_candidates = [\n",
    "    'app/javascript/fonts/roboto/roboto-medium-webfont.svg', #Complexity outlier, never changes\n",
    "    'app/models/status.rb', #Very high change rate somewhat high complexity\n",
    "    'app/javascript/styles/mastodon/components.scss', #Very high complexity, very high amount of changes\n",
    "    'app/javascript/mastodon/locales/ca.json',\n",
    "    'app/helpers/application_helper.rb',\n",
    "    'app/services/activitypub/process_status_update_service.rb']\n",
    "complexity_trends = compute_complexity_trends(hotspot_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Compute trends and add to dataframe\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_complexity_trend(index):\n",
    "    label, complexity_list = list(complexity_trends.items())[index]\n",
    "    commits_num = list(range(len(complexity_list)))\n",
    "    complexity_trend_line = px.line(x=commits_num, y=complexity_list, title=f\"Complexity trend for {label}\",\n",
    "                                    labels={'x': 'number of commits', 'y': 'complexity [LOC]'})\n",
    "    complexity_trend_line.show()\n",
    "    print(f\"Final complexity: {int(analysis_df['Complexity'][label])} LOC\" )\n",
    "    print(f\"Amount of changes: {analysis_df['Amount of changes'][label]}\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualize trends\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 1: app/javascript/fonts/roboto/roboto-medium-webfont.svg\n",
    "\n",
    "The 'roboto-medium-webfont.svg' file is an extreme outlier. It has very high complexity and never changes.\n",
    "From the file name we can easily see that it is not a troublesome hotspot.\n",
    "The file needs to have a lot of lines of code because it defines the shape of characters from a font.\n",
    "None of these lines ever need to change though. It is not a hotspot and we do not need to take any action."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 2: app/models/status.rb\n",
    "\n",
    "From the trend we see that the complexity increased gradually, with some decreases inbetween.\n",
    "Presumably these are refactorings, indicating that the file does indeed require some maintenance work."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 3: app/javascript/styles/mastodon/components.scss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 4: app/javascript/mastodon/locales/ca.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 5: app/helpers/application_helper.rb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 6: app/services/activitypub/process_status_update_service.rb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_trend(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Temporal/Logical Coupling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine what could be cases of temporal/logical coupling and generate a list\n",
    "of candidates with a set of coupled entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we consider logical grouping by commits.\n",
    "According to the [guidelines for pull requrests](https://github.com/mastodon/mastodon/blob/97f657f8181dc24f6c30b6e9f0ce52df827ac90f/CONTRIBUTING.md#pull-requests),\n",
    "small pull requests should be preferred, thus it is assumed that they contain only the necessary changes for one \"Add\", \"Change\", \"Deprecate\", \"Remove\" or \"Fix\",\n",
    "and can be viewed as logically grouped. To merge the pull request, a squash commit is added to the main branch. So for this analysis we can use the commits\n",
    "on the main branch to detect logical coupling between files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the sets of logically coupled is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChangeSet:\n",
    "    files: List[str]\n",
    "\n",
    "    def __init__(self, files: List[str]):\n",
    "        self.files=sorted(files)\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(\", \".join(self.files))\n",
    "\n",
    "    def __eq__(self, o: object) -> bool:\n",
    "        if not isinstance(o, ChangeSet):\n",
    "            return False\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return self.files == o.files\n",
    "\n",
    "    def to_tuple(self, nr_of_changes: int):\n",
    "        return self.files[0], self.files[1], nr_of_changes\n",
    "\n",
    "changed_together_filename = './changed_together.csv'\n",
    "\n",
    "col_name_nr_changed_together = 'nr_changed_together'\n",
    "changed_together_columns = ['file1', 'file2', col_name_nr_changed_together]\n",
    "dataframe_changed_together = DataFrame([], columns=changed_together_columns)\n",
    "\n",
    "if not path.exists(changed_together_filename):\n",
    "    files_changed_together = []\n",
    "    for commit in reversed(list(repo.traverse_commits())):\n",
    "        for file in commit.modified_files:\n",
    "            for changed_together in commit.modified_files:\n",
    "                if file.new_path is None or changed_together.new_path is None:\n",
    "                    continue\n",
    "                if not file.new_path == changed_together.new_path:\n",
    "                    files_changed_together.append(ChangeSet([file.new_path, changed_together.new_path]))\n",
    "    counter = Counter(files_changed_together)\n",
    "    changed_together_tuples = [change_set.to_tuple(nr_of_changes / 2) for change_set, nr_of_changes in counter.items()]\n",
    "    dataframe_changed_together = DataFrame(changed_together_tuples, columns=changed_together_columns)\n",
    "    dataframe_changed_together = dataframe_changed_together.sort_values(by=col_name_nr_changed_together, ascending=False)\n",
    "    dataframe_changed_together.to_csv(changed_together_filename)\n",
    "\n",
    "dataframe_changed_together = pd.read_csv(changed_together_filename, index_col=[0,1])\n",
    "\n",
    "\n",
    "\n",
    "print(f'most common changed together:')\n",
    "display(dataframe_changed_together[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the locales change often together. Thus we need to remove the locales from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_file_endings = ['.rb', '.js', '.haml', '.erb']\n",
    "\n",
    "def contains_allowed_file_type(change_set: List) -> bool:\n",
    "    for file in change_set:\n",
    "        ending = path.splitext(file)[1]\n",
    "        if allowed_file_endings.__contains__(ending):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "changed_together_filtered = [(row[1], row[2], row[3]) for row in dataframe_changed_together.to_records(index=True) if contains_allowed_file_type([row[1], row[2]])]\n",
    "dataframe_changed_together_filtered = DataFrame(changed_together_filtered, columns=changed_together_columns)\n",
    "\n",
    "display(dataframe_changed_together_filtered[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualize these candidate sets of couple entities with a visualization of your\n",
    "choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_hist = px.histogram(dataframe_changed_together_filtered, x='nr_changed_together', log_y=True)\n",
    "complexity_hist.update_layout(font_size=18)\n",
    "complexity_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_for_bar_plot = [(f'{row[1]} +  {row[2]}', row[3]) for row in dataframe_changed_together_filtered.to_records(index=True)]\n",
    "dataframe_for_bar_plot = DataFrame(list_for_bar_plot, columns=['files', col_name_nr_changed_together])\n",
    "\n",
    "bar = px.bar(dataframe_for_bar_plot[:20], x=col_name_nr_changed_together, y='files')\n",
    "bar.update_layout(font_size=18)\n",
    "bar.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For three set candidates in the list:\n",
    "• analyze and explain why these entities are coupled;\n",
    "• describe how important it would be to fix them, and any ideas for their\n",
    "improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canditate set 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canditate set 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate set 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Defective Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide on how you want to detect entities that had defects in the past (e.g.,\n",
    "commit message analysis vs. issue tracking system analysis) and motivate your\n",
    "choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine defective hotspots among the entities in the timeframe that you pre-\n",
    "viously selected (i.e., consider only defects in the selected timeframe). What\n",
    "conclusions can you draw from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine complexity hotspots at the beginning of your timeframe, then corre-\n",
    "late them with the defects they have presented throughout the entire timeframe.\n",
    "Is there a correlation? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What conclusions can you draw from the relationship between defective hotspots\n",
    "and complexity hotspots in Mastodon? And on these two metrics in general?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
